{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2664506d",
   "metadata": {},
   "source": [
    "Part-III: Ensemble Learning â€“ Bagging, Boosting, Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4641bd",
   "metadata": {},
   "source": [
    "Task 7: Conceptual Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cc13d",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1. What is the difference between Bagging and Boosting? \n",
    "2. How does Random Forest reduce variance? \n",
    "3. What is the weakness of boosting-based methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1519b46",
   "metadata": {},
   "source": [
    "Ans-1: \n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Aims to reduce variance and prevent overfitting.\n",
    "\n",
    "Trains multiple models independently and in parallel.\n",
    "\n",
    "Uses bootstrap sampling (random samples with replacement).\n",
    "\n",
    "Each model is trained on a different subset of data.\n",
    "\n",
    "Final prediction is made by majority voting (classification) or averaging (regression).\n",
    "\n",
    "Example: Random Forest.\n",
    "\n",
    "\n",
    "\n",
    "Boosting:\n",
    "\n",
    "Aims to reduce bias by converting weak learners into a strong learner.\n",
    "\n",
    "Models are trained sequentially, where each model learns from the errors of the previous one.\n",
    "\n",
    "Focuses more on misclassified instances, giving them higher weight in the next round.\n",
    "\n",
    "Final prediction is a weighted sum of all weak learners.\n",
    "\n",
    "More sensitive to overfitting and outliers if not tuned properly.\n",
    "\n",
    "Examples: AdaBoost, Gradient Boosting, XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea223ad",
   "metadata": {},
   "source": [
    "Ans-2: Random Forest is an ensemble method that builds multiple decision trees and combines their outputs to produce a more stable and accurate prediction. Here's how it reduces variance:\n",
    "\n",
    "Bootstrap Sampling: Each tree is trained on a different random subset of the training data. This introduces variety among the trees.\n",
    "\n",
    "Feature Randomness: At each split, a random subset of features is selected. This makes trees diverse and less correlated.\n",
    "\n",
    "Averaging (or Voting): The final prediction is made by aggregating the results of all individual trees. This smooths out the prediction and reduces the risk that a few highly variable trees dominate the output.\n",
    "\n",
    "This collective approach reduces the overall variance of the model without significantly increasing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca11166",
   "metadata": {},
   "source": [
    "Ans-3: While Boosting is powerful and often more accurate than bagging methods, it has some weaknesses:\n",
    "\n",
    "i)Sensitivity to Noisy Data and Outliers:\n",
    "\n",
    "A) Boosting focuses on correcting errors by giving more weight to misclassified points.\n",
    "\n",
    "B) If the data contains noise or outliers, boosting can overfit them, reducing generalization.\n",
    "\n",
    "ii)Longer Training Time:\n",
    "\n",
    "A) Models are built sequentially, so boosting is computationally slower compared to bagging.\n",
    "\n",
    "iii)Less Interpretability:\n",
    "\n",
    "A) Boosted models (like Gradient Boosting, XGBoost) are complex and hard to interpret compared to single decision trees.\n",
    "\n",
    "iv)Prone to Overfitting:\n",
    "\n",
    "A) If not properly tuned (e.g., with learning rate, number of estimators), boosting can overfit the training data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
