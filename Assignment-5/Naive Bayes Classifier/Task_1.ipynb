{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2380a4",
   "metadata": {},
   "source": [
    "Part-I: Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea995a41",
   "metadata": {},
   "source": [
    "Task 1: Theory Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c8e2f",
   "metadata": {},
   "source": [
    "Answer in 2â€“4 sentences: \n",
    "1. What is the core assumption of Naive Bayes? \n",
    "2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB. \n",
    "3. Why is Naive Bayes considered suitable for high-dimensional data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0500a41",
   "metadata": {},
   "source": [
    "Ans-1:-The core assumption of Naive Bayes is that all features are conditionally independent of each other given the class label. This means the presence or absence of one feature does not influence the others, which simplifies computation significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb8137",
   "metadata": {},
   "source": [
    "Ans-2:-GaussianNB is used when features are continuous and assumes they follow a normal distribution. MultinomialNB is suitable for discrete count data, like word frequencies in text classification. BernoulliNB is ideal for binary features, where each feature is either 0 or 1, representing absence or presence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b36d18",
   "metadata": {},
   "source": [
    "Ans-3:-Naive Bayes performs well in high-dimensional spaces because its assumption of feature independence reduces model complexity. This makes it efficient in terms of computation and memory, especially for tasks like text classification where the number of features (words) can be very large."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
