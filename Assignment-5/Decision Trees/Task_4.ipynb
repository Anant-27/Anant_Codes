{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6c3d17",
   "metadata": {},
   "source": [
    "Part-II: Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc31f7",
   "metadata": {},
   "source": [
    "Task 4: Conceptual Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148cc15",
   "metadata": {},
   "source": [
    "Answer briefly: \n",
    "1. What is entropy and information gain? \n",
    "2. Explain the difference between Gini Index and Entropy. \n",
    "3. How can a decision tree overfit? How can this be avoided? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a547ccc",
   "metadata": {},
   "source": [
    "Ans-1:Entropy is a metric used to represent the degree of impurity or uncertainty in a dataset. When a dataset has a perfect split (all values belong to one class), entropy is 0, meaning it’s completely pure. As the dataset becomes more mixed with different class labels, entropy increases, reaching a maximum when the classes are equally distributed.\n",
    "\n",
    "Information Gain refers to the decrease in entropy after a dataset is split based on a specific feature. It helps in selecting the feature that provides the most “useful” split by reducing uncertainty the most. A higher information gain means the attribute contributes more to improving classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55595017",
   "metadata": {},
   "source": [
    "Ans-2:| **Aspect**        | **Gini Index**                                   | **Entropy**                                             |\n",
    "| ----------------- | ------------------------------------------------ | ------------------------------------------------------- |\n",
    "| **Concept**       | Measures probability of incorrect classification | Measures randomness or impurity in information          |\n",
    "| **Formula**       | $Gini = 1 - \\sum p_i^2$                          | $Entropy = -\\sum p_i \\log_2(p_i)$                       |\n",
    "| **Computation**   | Faster (no logarithmic calculations)             | Slightly slower (involves log base 2)                   |\n",
    "| **Use in Models** | Commonly used in CART                            | Used in ID3 and C4.5 algorithms                         |\n",
    "| **Preference**    | Preferred for simplicity and speed               | Preferred when a theoretical info-based split is needed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441da4d2",
   "metadata": {},
   "source": [
    "Ans-3:Overfitting occurs when a decision tree becomes overly complex, trying to perfectly classify the training data—including outliers and noise. This hurts its performance on test data.\n",
    "\n",
    "Avoiding Overfitting:\n",
    "\n",
    "Pruning: Remove branches that do not contribute much to decision-making (can be done pre- or post-training).\n",
    "\n",
    "Max Depth: Limit how deep the tree can grow.\n",
    "\n",
    "Minimum Samples: Define the minimum number of samples required to split a node or be present at a leaf.\n",
    "\n",
    "Cross-Validation: Helps in selecting hyperparameters that generalize better.\n",
    "\n",
    "Ensemble Techniques: Use models like Random Forest or Gradient Boosting, which aggregate multiple trees and reduce variance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
